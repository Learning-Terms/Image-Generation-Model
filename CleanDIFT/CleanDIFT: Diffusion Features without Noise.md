# CleanDIFT: Diffusion Features without Noise
### [Arxiv](https://arxiv.org/pdf/2412.03439)   [Bili](https://www.bilibili.com/video/BV1b541197HX?buvid=XX68ACF6051B519928AE6B6F405AD2D005DC3&from_spmid=playlist.playlist-detail.0.0&is_story_h5=false&mid=DrRwFpk%2BbcbkpzyW8K9UaQ%3D%3D&plat_id=116&share_from=ugc&share_medium=android&share_plat=android&share_session_id=7ec5d6cc-eb1b-40d0-ae1d-b8d96e6a2a6f&share_source=WEIXIN&share_tag=s_i&spmid=united.player-video-detail.0.0&timestamp=1750868703&unique_k=9RxcO7z&up_id=373596439&share_source=weixin)


## Abstract
- **Internal features** from large-scale pre-trained diffusion models have recently been established as **powerful semantic descriptors** for **a wide range of downstream tasks**.
- Works that use these features generally need to **add noise to images before passing them through the model to obtain the semantic features**, as the models do not offer the most useful features when given images with little to no noise.
- We show that **this noise has a critical impact on the usefulness of these features that cannot be remedied by ensembling with different random noisesï¼ˆå™ªå£°çš„æ·»åŠ ä¸ºç‰¹å¾è´¨é‡å¸¦æ¥äº†ä¸ç¡®å®šå› ç´  æ½œåœ¨ç‰¹å¾çš„è´¨é‡å¯¹æ·»åŠ çš„å™ªå£°æ•æ„Ÿï¼‰**.
- We address this issue by **introducing a lightweight, unsupervised fine-tuning method that enables diffusion backbones to provide highquality, noise-free semantic features**.
- We show that these features readily outperform previous diffusion features by a wide margin in a wide variety of extraction setups and downstream tasks, offering better performance than even ensemble-based methods at a fraction of the cost.
------------
## æ‘˜è¦
- ğŸ“ ç ”ç©¶èƒŒæ™¯ï¼šå¤§è§„æ¨¡é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹ä¸­çš„ å†…éƒ¨ç‰¹å¾ï¼ˆintermediate featuresï¼‰è¿‘å¹´æ¥è¢«è¯æ˜å¯ä½œä¸ºå¼ºå¤§çš„è¯­ä¹‰æè¿°ç¬¦ï¼Œå¹¿æ³›ç”¨äºä¸‹æ¸¸ä»»åŠ¡ï¼ˆå¦‚å›¾åƒåˆ†ç±»ã€æ£€ç´¢ã€åˆ†å‰²ç­‰ï¼‰ã€‚
- ğŸš¨ é—®é¢˜å‘ç°ï¼šå½“å‰æå–è¿™äº›ç‰¹å¾çš„æ–¹æ³•é€šå¸¸éœ€è¦äººä¸ºç»™è¾“å…¥å›¾åƒåŠ å™ªå£°ï¼Œå› ä¸ºæ‰©æ•£æ¨¡å‹åœ¨å¤„ç†â€œå¹²å‡€å›¾åƒâ€æ—¶ï¼Œè¾“å‡ºçš„ç‰¹å¾ä¸å…·å¤‡è‰¯å¥½çš„è¯­ä¹‰è¡¨è¾¾åŠ›ã€‚**ä½†æ˜¯å™ªå£°çš„æ·»åŠ ä¸ºç‰¹å¾çš„è´¨é‡å¼•å…¥äº†ä¸ç¡®å®šæ€§ï¼Œä¸­é—´ç‰¹å¾å¯¹æ‰€æ·»åŠ çš„å™ªå£°æ•æ„Ÿ**ï¼Œå³ä½¿ä½¿ç”¨å¤šä¸ªéšæœºå™ªå£°è¿›è¡Œ ensembleï¼ˆé›†æˆï¼‰ï¼Œä¹Ÿæ— æ³•æœ‰æ•ˆè§£å†³ç‰¹å¾è´¨é‡é—®é¢˜ã€‚
- ğŸ’¡ ä½œè€…æå‡ºçš„æ–¹æ³•ï¼šæå‡ºä¸€ç§ è½»é‡çº§çš„ã€æ— ç›‘ç£çš„ fine-tuning æ–¹æ³•ï¼Œå¯¹ diffusion backbone è¿›è¡Œå¾®è°ƒï¼Œä½¿å…¶åœ¨ æ— å™ªå£°è¾“å…¥ä¸‹ä¹Ÿèƒ½è¾“å‡ºé«˜è´¨é‡çš„è¯­ä¹‰ç‰¹å¾ã€‚
- ğŸ“ˆ æ•ˆæœä¸è´¡çŒ®ï¼šè¯¥æ–¹æ³•åœ¨å¤šç§ç‰¹å¾æå–åœºæ™¯å’Œä¸‹æ¸¸ä»»åŠ¡ä¸­ï¼Œæ˜¾è‘—ä¼˜äºåŸå§‹çš„æ‰©æ•£ç‰¹å¾ï¼ˆå³å¸¦å™ªå£°æå–çš„ç‰¹å¾ï¼‰ã€‚å¹¶ä¸”ä¸ä¾èµ– ensemble æŠ€æœ¯ï¼Œè€Œåœ¨æ•ˆæœä¸Šä¼˜äº ensemble æ–¹æ³•ï¼Œä¸”è®¡ç®—æˆæœ¬æ›´ä½ã€‚
-------------
## æŠ€æœ¯æœ¯è¯­ä¸ç†è®ºè§£é‡Š
### ğŸŒ€ Diffusion Modelsï¼ˆæ‰©æ•£æ¨¡å‹ï¼‰
- æ‰©æ•£æ¨¡å‹æ˜¯ä¸€ç±»ç”Ÿæˆæ¨¡å‹ï¼Œå…¶è®­ç»ƒæµç¨‹åŒ…æ‹¬ï¼š
  - æ­£å‘è¿‡ç¨‹ï¼ˆForward Processï¼‰ï¼šé€æ­¥å¾€å›¾åƒä¸­æ·»åŠ é«˜æ–¯å™ªå£°
  - åå‘è¿‡ç¨‹ï¼ˆReverse Processï¼‰ï¼šé€šè¿‡ç¥ç»ç½‘ç»œå­¦ä¹ å¦‚ä½•é€æ­¥å»å™ªï¼Œè¿˜åŸåŸå§‹å›¾åƒ
  - DDPMï¼šDenoising Diffusion Probabilistic Models (Ho et al., 2020)

### ğŸ¯ Intermediate Semantic Featuresï¼ˆä¸­é—´è¯­ä¹‰ç‰¹å¾ï¼‰
- åœ¨æ‰©æ•£æ¨¡å‹ä¸­ï¼Œç‰¹å¾é€šå¸¸æŒ‡ U-Net ç½‘ç»œç»“æ„ä¸­ä¸åŒå±‚çš„ä¸­é—´è¾“å‡ºã€‚
- ç ”ç©¶å‘ç°ï¼Œè¿™äº›ä¸­é—´ç‰¹å¾èƒ½**æœ‰æ•ˆç¼–ç ï¼ˆä½“ç°ï¼‰è¾“å…¥å›¾åƒçš„é«˜å±‚è¯­ä¹‰ä¿¡æ¯**ï¼Œä¸ CLIP ç­‰æ¨¡å‹ä¸­çš„â€œå›¾åƒåµŒå…¥â€æœ‰ç±»ä¼¼ç”¨é€”ã€‚

### âŒ ä¸ºä»€ä¹ˆéœ€è¦åŠ å™ªå£°æ‰èƒ½æå–è¯­ä¹‰ç‰¹å¾ï¼Ÿ
- æ‰©æ•£æ¨¡å‹çš„ U-Net é€šå¸¸æ˜¯ä¸ºâ€œå™ªå£°è¾“å…¥â€è®¾è®¡çš„ï¼Œå› æ­¤åœ¨è¾“å…¥å›¾åƒå‡ ä¹æ²¡æœ‰å™ªå£°æ—¶ï¼Œå…¶å¤„ç†æ–¹å¼**ä¸è®­ç»ƒé˜¶æ®µå­˜åœ¨åˆ†å¸ƒåå·®ï¼ˆdistribution shiftï¼‰**ï¼Œå¯¼è‡´ï¼š
  - ç‰¹å¾è¡¨è¾¾å¼±
  - æ¨¡å‹æ¿€æ´»ä¸å……åˆ†
- åŠ å™ªå£°æ˜¯äººä¸ºâ€œæ¨¡æ‹Ÿè®­ç»ƒæ—¶çš„è¾“å…¥â€ï¼Œ**ä½†è¿™ä¼šå¼•å…¥ä¸ç¨³å®šæ€§ï¼ˆå¯¹éšæœºå™ªå£°æ•æ„Ÿï¼‰**

### ğŸ› ï¸ ä½œè€…æå‡ºçš„ Unsupervised Fine-tuning æ–¹æ³•
- æ— éœ€æ ‡æ³¨æ•°æ®ï¼Œé€šè¿‡è°ƒæ•´æ‰©æ•£æ¨¡å‹å‚æ•°ï¼Œä½¿å…¶åœ¨â€œæ— å™ªå£°è¾“å…¥â€ä¸‹ä¹Ÿèƒ½æå–é«˜è´¨é‡è¯­ä¹‰ç‰¹å¾ã€‚
- ç‰¹ç‚¹ï¼š
  - è½»é‡ï¼šå¾®è°ƒå‚æ•°é‡è¾ƒå°
  - æ— ç›‘ç£ï¼šæ— éœ€ä¸‹æ¸¸ä»»åŠ¡çš„æ ‡ç­¾
  - æ•ˆç‡é«˜ï¼šå»é™¤ ensemble å¸¦æ¥çš„å†—ä½™è®¡ç®—
------------

## Introduction
- Learning meaningful visual representations that capture a vast amount of world knowledge remains a key problem in the field of computer vision. **Diffusion models can be trained at scale in a self-supervised manner** and have rapidly advanced the state of the art in image [12, 41, 51] and video generation [13, 25, 58], making them a good candidate to learn visual representations.
- Many early works have already achieved impressive results using internal features from large-scale pretrained diffusion models for a wide variety of tasks, such as semantic correspondence detection [62, 66, 67], semantic segmentation [2, 39, 63], panoptic segmentation [64], object detection [8], and classification [31].
- However, <mark>the optimal approach to extract this world knowledge from a diffusion model remains uncertain</mark>.
---------
- To understand why that is the case, we take a look at how diffusion models are trained: <mark>a varying amount of noise is added to a clean input image (forward process) and the model is tasked to remove the noise from the image (backward process)</mark>.
- The amount of added noise is dependent on **the diffusion timestep**. As a result, the model learns to operate on noisy images and also **becomes dependent(ä¾èµ–) on the noise timestep** as different noise levels require the model to perform different tasks [1, 4].
- Since noisy images inherently contain less information than clean images (cf. Figure 2), we hypothesize that **this harms the internal feature representation** of diffusion models [9] and, thus, the extractable world knowledge.
- Furthermore, the timestep acts as a hyperparameter that influences the internal feature representation and needs to be picked independently for every downstream application (cf. Figure 14).
-----------
## å¯¼å¼•ç¬¬ä¸€èŠ‚
### ğŸ§  ä¸€å¥è¯æ€»ç»“
- è™½ç„¶å¤§è§„æ¨¡é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒå’Œè§†é¢‘ç”Ÿæˆä¸Šå–å¾—çªç ´ï¼Œå¹¶èƒ½ç”¨äºå¤šç§è§†è§‰ä»»åŠ¡ï¼Œä½†å¦‚ä½•æœ€æœ‰æ•ˆåœ°æå–å…¶å†…éƒ¨è¯­ä¹‰ç‰¹å¾ï¼ˆä¸–ç•ŒçŸ¥è¯†ï¼‰ä»æ˜¯æœªè§£é—®é¢˜ï¼Œå…¶æ ¹æºåœ¨äºï¼šæ¨¡å‹æ˜¯åœ¨æœ‰å™ªå£°è¾“å…¥ä¸Šè®­ç»ƒçš„ï¼Œå¯¼è‡´å…¶å†…éƒ¨ç‰¹å¾è´¨é‡ä¸å™ªå£°æ°´å¹³å¯†åˆ‡ç›¸å…³ï¼Œè¿›è€Œå½±å“ç‰¹å¾å¯ç”¨æ€§å’Œç¨³å®šæ€§ã€‚

### ğŸ§ª ä½œè€…çš„å‡è®¾ï¼ˆhypothesisï¼‰
- å› ä¸ºå™ªå£°å›¾åƒæœ¬èº«ä¿¡æ¯é‡å°‘äºåŸå§‹å›¾åƒï¼Œæ¨¡å‹å­¦åˆ°çš„å†…éƒ¨è¡¨ç¤ºå¯èƒ½ä¹Ÿå—é™
- åŠ å™ªç¨‹åº¦ï¼ˆtimestepï¼‰å˜æˆäº†ä¸€ä¸ªé«˜æ•æ„Ÿçš„è¶…å‚æ•°ï¼Œå› ä¸ºä¸åŒä»»åŠ¡éœ€è¦é€‰æ‹©ä¸åŒ timestep æ¥æ·»åŠ å™ªå£° ä»¥è®©æ‰©æ•£æ¨¡å‹æå–ç‰¹å¾ â†’ å¯è¿ç§»æ€§å·®ï¼Œè°ƒå‚æˆæœ¬é«˜
- **å› æ­¤éœ€è¦è§£å†³çš„é—®é¢˜å¦‚ä¸‹ï¼šå™ªå£°æŸå®³è¯­ä¹‰è¡¨ç¤º + timestepè¶…å‚æ•°æ•æ„Ÿ**

###  ğŸ“Š å°ç»“å›¾ç¤º
```
æ‰©æ•£æ¨¡å‹è®­ç»ƒè¾“å…¥ = åŸå§‹å›¾åƒ + ï¼ˆå¯¹äºä¸åŒä»»åŠ¡ï¼‰ä¸åŒçš„timestepæ·»åŠ å™ªå£°
        â†“
æ•…æ¨¡å‹ç‰¹å¾ä¾èµ–äºå™ªå£°ç¨‹åº¦å’Œtimestep
        â†“
ğŸš¨ å¾…è§£å†³é—®é¢˜:å™ªå£°æŸå®³è¯­ä¹‰è¡¨ç¤º + timestepè¶…å‚æ•°æ•æ„Ÿ
        â†“
å¯¼è‡´ä¸‹æ¸¸ä»»åŠ¡æå–ç‰¹å¾æ—¶ä¸ç¨³å®š & éš¾ä»¥æ³›åŒ–
```
------------
- We propose a novel feature extraction method that (1) eliminates the need to destroy information by adding noise to the input; and (2) produces timestep-independent generic diffusion features useful for a wide range of down-stream
tasks, alleviating the need to tune a noising timestep per down-stream task.
- We show how to adapt an off-the-shelf large-scale pre-trained diffusion backbone to provide these features at minimal cost (approximately 30 minutes of finetuning on a single A100 GPU) and demonstrate improved performance across a wide range of downstream tasks.
- We achieve this by viewing a diffusion model as a family of T feature extractors that operate on images with different noise levels and provide features with different characteristics. We consolidate all T feature extraction functions in ourfeature extractor by aligning their internal representations.
- Specifically, we initialize our feature extractor as a trainable copy of the diffusion model; fine-tune it with clean images and no timestep input; and align its features with all T timedependent feature extractors of the diffusion model.
- We evaluate our improved features across a wide variety of downstream tasks, such as semantic correspondence matching, monocular depth estimation, semantic segmentation, and classification, and find that they consistently improve upon approaches based on standard diffusion features. These improvements are most evident for dense visual tasks such as semantic correspondence matching, where our features show substantial performance gains across a wide variety of setups [62, 66, 67] and set a new state-of-the-art for unsupervised semantic correspondence matching.
- Additionally, our proposed method eliminates the need for noise or timestep ensembling [62], offering substantial speed gains (e.g., 8Ë† over DIFT [62]), on top of improved quality. Our method is generic and integrates easily with established methods, such as fusing diffusion and DINOv2 features [66, 67].


