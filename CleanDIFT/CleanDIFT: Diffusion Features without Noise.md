# CleanDIFT: Diffusion Features without Noise
### [Arxiv](https://arxiv.org/pdf/2412.03439)   [Bili](https://www.bilibili.com/video/BV1b541197HX?buvid=XX68ACF6051B519928AE6B6F405AD2D005DC3&from_spmid=playlist.playlist-detail.0.0&is_story_h5=false&mid=DrRwFpk%2BbcbkpzyW8K9UaQ%3D%3D&plat_id=116&share_from=ugc&share_medium=android&share_plat=android&share_session_id=7ec5d6cc-eb1b-40d0-ae1d-b8d96e6a2a6f&share_source=WEIXIN&share_tag=s_i&spmid=united.player-video-detail.0.0&timestamp=1750868703&unique_k=9RxcO7z&up_id=373596439&share_source=weixin)


## Abstract
- **Internal features** from large-scale pre-trained diffusion models have recently been established as **powerful semantic descriptors** for **a wide range of downstream tasks**.
- Works that use these features generally need to **add noise to images before passing them through the model to obtain the semantic features**, as the models do not offer the most useful features when given images with little to no noise.
- We show that **this noise has a critical impact on the usefulness of these features that cannot be remedied by ensembling with different random noisesï¼ˆå™ªå£°çš„æ·»åŠ ä¸ºç‰¹å¾è´¨é‡å¸¦æ¥äº†ä¸ç¡®å®šå› ç´  æ½œåœ¨ç‰¹å¾çš„è´¨é‡å¯¹æ·»åŠ çš„å™ªå£°æ•æ„Ÿï¼‰**.
- We address this issue by **introducing a lightweight, unsupervised fine-tuning method that enables diffusion backbones to provide highquality, noise-free semantic features**.
- We show that these features readily outperform previous diffusion features by a wide margin in a wide variety of extraction setups and downstream tasks, offering better performance than even ensemble-based methods at a fraction of the cost.
------------
## æ‘˜è¦
- ğŸ“ ç ”ç©¶èƒŒæ™¯ï¼šå¤§è§„æ¨¡é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹ä¸­çš„ å†…éƒ¨ç‰¹å¾ï¼ˆintermediate featuresï¼‰è¿‘å¹´æ¥è¢«è¯æ˜å¯ä½œä¸ºå¼ºå¤§çš„è¯­ä¹‰æè¿°ç¬¦ï¼Œå¹¿æ³›ç”¨äºä¸‹æ¸¸ä»»åŠ¡ï¼ˆå¦‚å›¾åƒåˆ†ç±»ã€æ£€ç´¢ã€åˆ†å‰²ç­‰ï¼‰ã€‚
- ğŸš¨ é—®é¢˜å‘ç°ï¼šå½“å‰æå–è¿™äº›ç‰¹å¾çš„æ–¹æ³•é€šå¸¸éœ€è¦äººä¸ºç»™è¾“å…¥å›¾åƒåŠ å™ªå£°ï¼Œå› ä¸ºæ‰©æ•£æ¨¡å‹åœ¨å¤„ç†â€œå¹²å‡€å›¾åƒâ€æ—¶ï¼Œè¾“å‡ºçš„ç‰¹å¾ä¸å…·å¤‡è‰¯å¥½çš„è¯­ä¹‰è¡¨è¾¾åŠ›ã€‚**ä½†æ˜¯å™ªå£°çš„æ·»åŠ ä¸ºç‰¹å¾çš„è´¨é‡å¼•å…¥äº†ä¸ç¡®å®šæ€§ï¼Œä¸­é—´ç‰¹å¾å¯¹æ‰€æ·»åŠ çš„å™ªå£°æ•æ„Ÿ**ï¼Œå³ä½¿ä½¿ç”¨å¤šä¸ªéšæœºå™ªå£°è¿›è¡Œ ensembleï¼ˆé›†æˆï¼‰ï¼Œä¹Ÿæ— æ³•æœ‰æ•ˆè§£å†³ç‰¹å¾è´¨é‡é—®é¢˜ã€‚
- ğŸ’¡ ä½œè€…æå‡ºçš„æ–¹æ³•ï¼šæå‡ºä¸€ç§ è½»é‡çº§çš„ã€æ— ç›‘ç£çš„ fine-tuning æ–¹æ³•ï¼Œå¯¹ diffusion backbone è¿›è¡Œå¾®è°ƒï¼Œä½¿å…¶åœ¨ æ— å™ªå£°è¾“å…¥ä¸‹ä¹Ÿèƒ½è¾“å‡ºé«˜è´¨é‡çš„è¯­ä¹‰ç‰¹å¾ã€‚
- ğŸ“ˆ æ•ˆæœä¸è´¡çŒ®ï¼šè¯¥æ–¹æ³•åœ¨å¤šç§ç‰¹å¾æå–åœºæ™¯å’Œä¸‹æ¸¸ä»»åŠ¡ä¸­ï¼Œæ˜¾è‘—ä¼˜äºåŸå§‹çš„æ‰©æ•£ç‰¹å¾ï¼ˆå³å¸¦å™ªå£°æå–çš„ç‰¹å¾ï¼‰ã€‚å¹¶ä¸”ä¸ä¾èµ– ensemble æŠ€æœ¯ï¼Œè€Œåœ¨æ•ˆæœä¸Šä¼˜äº ensemble æ–¹æ³•ï¼Œä¸”è®¡ç®—æˆæœ¬æ›´ä½ã€‚
-------------
## æŠ€æœ¯æœ¯è¯­ä¸ç†è®ºè§£é‡Š
### ğŸŒ€ Diffusion Modelsï¼ˆæ‰©æ•£æ¨¡å‹ï¼‰
- æ‰©æ•£æ¨¡å‹æ˜¯ä¸€ç±»ç”Ÿæˆæ¨¡å‹ï¼Œå…¶è®­ç»ƒæµç¨‹åŒ…æ‹¬ï¼š
  - æ­£å‘è¿‡ç¨‹ï¼ˆForward Processï¼‰ï¼šé€æ­¥å¾€å›¾åƒä¸­æ·»åŠ é«˜æ–¯å™ªå£°
  - åå‘è¿‡ç¨‹ï¼ˆReverse Processï¼‰ï¼šé€šè¿‡ç¥ç»ç½‘ç»œå­¦ä¹ å¦‚ä½•é€æ­¥å»å™ªï¼Œè¿˜åŸåŸå§‹å›¾åƒ
  - DDPMï¼šDenoising Diffusion Probabilistic Models (Ho et al., 2020)

### ğŸ¯ Intermediate Semantic Featuresï¼ˆä¸­é—´è¯­ä¹‰ç‰¹å¾ï¼‰
- åœ¨æ‰©æ•£æ¨¡å‹ä¸­ï¼Œç‰¹å¾é€šå¸¸æŒ‡ U-Net ç½‘ç»œç»“æ„ä¸­ä¸åŒå±‚çš„ä¸­é—´è¾“å‡ºã€‚
- ç ”ç©¶å‘ç°ï¼Œè¿™äº›ä¸­é—´ç‰¹å¾èƒ½**æœ‰æ•ˆç¼–ç ï¼ˆä½“ç°ï¼‰è¾“å…¥å›¾åƒçš„é«˜å±‚è¯­ä¹‰ä¿¡æ¯**ï¼Œä¸ CLIP ç­‰æ¨¡å‹ä¸­çš„â€œå›¾åƒåµŒå…¥â€æœ‰ç±»ä¼¼ç”¨é€”ã€‚

### âŒ ä¸ºä»€ä¹ˆéœ€è¦åŠ å™ªå£°æ‰èƒ½æå–è¯­ä¹‰ç‰¹å¾ï¼Ÿ
- æ‰©æ•£æ¨¡å‹çš„ U-Net é€šå¸¸æ˜¯ä¸ºâ€œå™ªå£°è¾“å…¥â€è®¾è®¡çš„ï¼Œå› æ­¤åœ¨è¾“å…¥å›¾åƒå‡ ä¹æ²¡æœ‰å™ªå£°æ—¶ï¼Œå…¶å¤„ç†æ–¹å¼**ä¸è®­ç»ƒé˜¶æ®µå­˜åœ¨åˆ†å¸ƒåå·®ï¼ˆdistribution shiftï¼‰**ï¼Œå¯¼è‡´ï¼š
  - ç‰¹å¾è¡¨è¾¾å¼±
  - æ¨¡å‹æ¿€æ´»ä¸å……åˆ†
- åŠ å™ªå£°æ˜¯äººä¸ºâ€œæ¨¡æ‹Ÿè®­ç»ƒæ—¶çš„è¾“å…¥â€ï¼Œ**ä½†è¿™ä¼šå¼•å…¥ä¸ç¨³å®šæ€§ï¼ˆå¯¹éšæœºå™ªå£°æ•æ„Ÿï¼‰**

### ğŸ› ï¸ ä½œè€…æå‡ºçš„ Unsupervised Fine-tuning æ–¹æ³•
- æ— éœ€æ ‡æ³¨æ•°æ®ï¼Œé€šè¿‡è°ƒæ•´æ‰©æ•£æ¨¡å‹å‚æ•°ï¼Œä½¿å…¶åœ¨â€œæ— å™ªå£°è¾“å…¥â€ä¸‹ä¹Ÿèƒ½æå–é«˜è´¨é‡è¯­ä¹‰ç‰¹å¾ã€‚
- ç‰¹ç‚¹ï¼š
  - è½»é‡ï¼šå¾®è°ƒå‚æ•°é‡è¾ƒå°
  - æ— ç›‘ç£ï¼šæ— éœ€ä¸‹æ¸¸ä»»åŠ¡çš„æ ‡ç­¾
  - æ•ˆç‡é«˜ï¼šå»é™¤ ensemble å¸¦æ¥çš„å†—ä½™è®¡ç®—
------------

## Introduction
- Learning meaningful visual representations that capture a vast amount of world knowledge remains a key problem in the field of computer vision. **Diffusion models can be trained at scale in a self-supervised manner** and have rapidly advanced the state of the art in image [12, 41, 51] and video generation [13, 25, 58], making them a good candidate to learn visual representations.
- Many early works have already achieved impressive results using internal features from large-scale pretrained diffusion models for a wide variety of tasks, such as semantic correspondence detection [62, 66, 67], semantic segmentation [2, 39, 63], panoptic segmentation [64], object detection [8], and classification [31].
- However, <mark>the optimal approach to extract this world knowledge from a diffusion model remains uncertain</mark>.
---------
- To understand why that is the case, we take a look at how diffusion models are trained: <mark>a varying amount of noise is added to a clean input image (forward process) and the model is tasked to remove the noise from the image (backward process)</mark>.
- The amount of added noise is dependent on **the diffusion timestep**. As a result, the model learns to operate on noisy images and also **becomes dependent(ä¾èµ–) on the noise timestep** as different noise levels require the model to perform different tasks [1, 4].
- Since noisy images inherently contain less information than clean images (cf. Figure 2), we hypothesize that **this harms the internal feature representation** of diffusion models [9] and, thus, the extractable world knowledge.
- Furthermore, the timestep acts as a hyperparameter that influences the internal feature representation and needs to be picked independently for every downstream application (cf. Figure 14).
-----------
## å¯¼å¼•ç¬¬ä¸€èŠ‚
### ğŸ§  ä¸€å¥è¯æ€»ç»“
- è™½ç„¶å¤§è§„æ¨¡é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒå’Œè§†é¢‘ç”Ÿæˆä¸Šå–å¾—çªç ´ï¼Œå¹¶èƒ½ç”¨äºå¤šç§è§†è§‰ä»»åŠ¡ï¼Œä½†å¦‚ä½•æœ€æœ‰æ•ˆåœ°æå–å…¶å†…éƒ¨è¯­ä¹‰ç‰¹å¾ï¼ˆä¸–ç•ŒçŸ¥è¯†ï¼‰ä»æ˜¯æœªè§£é—®é¢˜ï¼Œå…¶æ ¹æºåœ¨äºï¼šæ¨¡å‹æ˜¯åœ¨æœ‰å™ªå£°è¾“å…¥ä¸Šè®­ç»ƒçš„ï¼Œå¯¼è‡´å…¶å†…éƒ¨ç‰¹å¾è´¨é‡ä¸å™ªå£°æ°´å¹³å¯†åˆ‡ç›¸å…³ï¼Œè¿›è€Œå½±å“ç‰¹å¾å¯ç”¨æ€§å’Œç¨³å®šæ€§ã€‚

### ğŸ§ª ä½œè€…çš„å‡è®¾ï¼ˆhypothesisï¼‰
- å› ä¸ºå™ªå£°å›¾åƒæœ¬èº«ä¿¡æ¯é‡å°‘äºåŸå§‹å›¾åƒï¼Œæ¨¡å‹å­¦åˆ°çš„å†…éƒ¨è¡¨ç¤ºå¯èƒ½ä¹Ÿå—é™
- åŠ å™ªç¨‹åº¦ï¼ˆtimestepï¼‰å˜æˆäº†ä¸€ä¸ªé«˜æ•æ„Ÿçš„è¶…å‚æ•°ï¼Œå› ä¸ºä¸åŒä»»åŠ¡éœ€è¦é€‰æ‹©ä¸åŒ timestep æ¥æ·»åŠ å™ªå£° ä»¥è®©æ‰©æ•£æ¨¡å‹æå–ç‰¹å¾ â†’ å¯è¿ç§»æ€§å·®ï¼Œè°ƒå‚æˆæœ¬é«˜
- **å› æ­¤éœ€è¦è§£å†³çš„é—®é¢˜å¦‚ä¸‹ï¼šå™ªå£°æŸå®³è¯­ä¹‰è¡¨ç¤º + timestepè¶…å‚æ•°æ•æ„Ÿ**

###  ğŸ“Š å°ç»“å›¾ç¤º
```
æ‰©æ•£æ¨¡å‹è®­ç»ƒè¾“å…¥ = åŸå§‹å›¾åƒ + ï¼ˆå¯¹äºä¸åŒä»»åŠ¡ï¼‰è®¾å®šä¸åŒçš„timestepæ·»åŠ å™ªå£°
        â†“
æ•…æ¨¡å‹ç‰¹å¾ä¾èµ–äºå™ªå£°ç¨‹åº¦å’Œtimestep
        â†“
ğŸš¨ å¾…è§£å†³é—®é¢˜:å™ªå£°æŸå®³è¯­ä¹‰è¡¨ç¤º + timestepè¶…å‚æ•°æ•æ„Ÿ
        â†“
å¯¼è‡´ä¸‹æ¸¸ä»»åŠ¡æå–ç‰¹å¾æ—¶ä¸ç¨³å®š & éš¾ä»¥æ³›åŒ–
```
------------
- We propose a **novel feature extraction method** that <mark>(1) eliminates the need to destroy information by adding noise to the input; and (2) produces **timestep-independent** generic diffusion features useful for a wide range of down-stream tasks, alleviating the need to tune a noising timestep per down-stream task</mark>.
- We show how to **adapt an off-the-shelf large-scale pre-trained diffusion backbone to provide these features at minimal cost** (approximately 30 minutes of finetuning on a single A100 GPU) and demonstrate improved performance across a wide range of downstream tasks.
- We achieve this by viewing a diffusion model as a family of T feature extractors that operate on images with different noise levels and provide features with different characteristics. We consolidate all T feature extraction functions in ourfeature extractor by aligning their internal representations.
- Specifically, we initialize our feature extractor as a trainable copy of the diffusion model; fine-tune it with clean images and no timestep input; and align its features with all T timedependent feature extractors of the diffusion model.
- We evaluate our improved features across a wide variety of downstream tasks, such as semantic correspondence matching, monocular depth estimation, semantic segmentation, and classification, and find that they consistently improve upon approaches based on standard diffusion features. These improvements are most evident for dense visual tasks such as semantic correspondence matching, where our features show substantial performance gains across a wide variety of setups [62, 66, 67] and set a new state-of-the-art for unsupervised semantic correspondence matching.
- Additionally, our proposed method eliminates the need for noise or timestep ensembling [62], offering substantial speed gains (e.g., 8Ë† over DIFT [62]), on top of improved quality. Our method is generic and integrates easily with established methods, such as fusing diffusion and DINOv2 features [66, 67].
------------------
## å¯¼å¼•ç¬¬äºŒèŠ‚
### ğŸ§  ä¸€å¥è¯æ€»ç»“
- ä½œè€…æå‡ºäº†ä¸€ç§**æ— éœ€åŠ å™ªã€timestepæ— å…³**çš„**åŸºäºæ‰©æ•£æ¨¡å‹çš„ç‰¹å¾æå–æ–¹æ³•**ï¼Œé€šè¿‡**è½»é‡å¾®è°ƒ**å³å¯è·å¾—é«˜è´¨é‡ã€é€šç”¨ã€é€Ÿåº¦æ›´å¿«çš„è§†è§‰è¡¨ç¤ºï¼Œå¹¿æ³›æå‡ä¸‹æ¸¸ä»»åŠ¡è¡¨ç°ï¼Œå°¤å…¶åœ¨è¯­ä¹‰åŒ¹é…ç­‰å¯†é›†ä»»åŠ¡ä¸­è¾¾åˆ°SOTAæ°´å¹³ã€‚

### ğŸ“Œ æ ¸å¿ƒåˆ›æ–°ç‚¹
| é¡¹ç›®          | å†…å®¹                                                        |
| ----------- | --------------------------------------------------------- |
| ğŸ¯ **ç›®æ ‡**   | ä»æ‰©æ•£æ¨¡å‹ä¸­æå–**é«˜è´¨é‡ã€é€šç”¨ã€ä¸ä¾èµ–å™ªå£°æˆ–timestep**çš„è¯­ä¹‰ç‰¹å¾                    |
| ğŸ§ª **é—®é¢˜**   | ä¼ ç»Ÿæ–¹æ³•éœ€åŠ å™ª + è°ƒæ•´ timestepï¼Œä¸”ç‰¹å¾å¯¹ timestep æ•æ„Ÿ                    |
| ğŸ’¡ **æ–¹æ³•**   | æå‡ºä¸€ç§æ–°çš„ç‰¹å¾æå–æ–¹æ³•ï¼Œä½¿æ¨¡å‹èƒ½åœ¨**æ— å™ªå£°+æ— timestepè¾“å…¥**ä¸‹æå–ç‰¹å¾                |
| ğŸ”§ **å®ç°ç­–ç•¥** | è§†æ‰©æ•£æ¨¡å‹ä¸ºç”±å¤šä¸ª timestep ç‰¹å¾æå–å™¨ç»„æˆçš„é›†åˆï¼Œé€šè¿‡**ç‰¹å¾å¯¹é½**å°†è¿™äº›ç‰¹å¾æ•´åˆè¿›ä¸€ä¸ªå•ä¸€æ¨¡å‹    |
| âš™ï¸ **è®­ç»ƒæ­¥éª¤** | å¾®è°ƒä¸€ä¸ªå¯è®­ç»ƒå‰¯æœ¬ï¼Œä½¿ç”¨å¹²å‡€å›¾åƒã€æ—  timestep è¾“å…¥ï¼Œå°†å…¶ç‰¹å¾å¯¹é½åˆ°åŸæ¨¡å‹ä¸åŒ timestep çš„ç‰¹å¾ä¸Š |
| ğŸ§© **é€‚é…æ€§**  | æ–¹æ³•è½»é‡ï¼ˆ30åˆ†é’Ÿå¾®è°ƒï¼‰ï¼Œå…¼å®¹å·²æœ‰æ¶æ„ï¼ˆå¦‚ DINOv2ï¼‰ï¼Œæ˜“é›†æˆ                         |
| ğŸš€ **ä¼˜åŠ¿**   | æ— éœ€åŠ å™ªæˆ– ensembleã€æ¨ç†é€Ÿåº¦å¿«ï¼ˆæ¯” DIFT å¿«çº¦ 8 å€ï¼‰ã€è¡¨ç°æ›´å¥½                  |
| ğŸ§  **æ•ˆæœéªŒè¯** | å¤šä¸ªä»»åŠ¡éªŒè¯ï¼ˆè¯­ä¹‰åŒ¹é…ã€æ·±åº¦ä¼°è®¡ã€åˆ†å‰²ã€åˆ†ç±»ï¼‰ï¼Œ**åœ¨å¯†é›†è§†è§‰ä»»åŠ¡ä¸Šæ˜¾è‘—é¢†å…ˆ**                  |

### ğŸ” å…³é”®æŠ€æœ¯ç‚¹è®²è§£
#### 1. âœ… æ— éœ€åŠ å™ªã€æ— timestepè¾“å…¥
- åŸå§‹æ‰©æ•£æ¨¡å‹æ˜¯â€œç»™å®šå™ªå£°ç¨‹åº¦ + å›¾åƒâ€â†’ æå–ç‰¹å¾(ç»™å›¾åƒæ·»åŠ å™ªå£°çš„è¿‡ç¨‹ä¸­éœ€è¦è®¾ç½®timestep)
- ä½œè€…æå‡ºï¼šç›´æ¥ç”¨å¹²å‡€å›¾åƒ+ä¸è¾“å…¥ timestepï¼Œä¹Ÿèƒ½æå–è¯­ä¹‰ç‰¹å¾
- é€šè¿‡**ç‰¹å¾å¯¹é½**å®ç°è¿™ä¸€èƒ½åŠ›

#### 2. ğŸ§  å°†æ‰©æ•£æ¨¡å‹è§†ä½œä¸€ä¸ªâ€œTæ­¥ç‰¹å¾æå–å™¨é›†åˆâ€
- åŸå§‹æ¨¡å‹å¯¹æ¯ä¸ª timestep éƒ½å­¦ä¼šâ€œä¸åŒçš„ç‰¹å¾æå–ä»»åŠ¡â€
- ä½œè€…å°†æ‰€æœ‰ timestep çš„ç‰¹å¾è¾“å‡ºè§†ä½œâ€œä¸€ä¸ªé›†åˆâ€ï¼Œç„¶åï¼š
  **è®­ç»ƒä¸€ä¸ªåŸºäºæ‰©æ•£æ¨¡å‹çš„ç»Ÿä¸€æ¨¡å‹ï¼Œä½¿å¾—è¿™ä¸ªæ¨¡å‹è¾“å‡ºçš„ç‰¹å¾èƒ½å¤Ÿ å¯¹é½æ‰€æœ‰è¿™äº› timestep çš„ç‰¹å¾è¾“å‡º**

#### 3. âš™ï¸ è®­ç»ƒæ–¹æ³•ç»†èŠ‚
- åˆå§‹åŒ–ä¸º diffusion backbone çš„å‰¯æœ¬ï¼ˆå¯è®­ç»ƒï¼‰
- ç”¨å¹²å‡€å›¾åƒè®­ç»ƒï¼Œä¸åŠ å™ªå£°ã€ä¸è¾“å…¥ timestep
- è®­ç»ƒç›®æ ‡ï¼šä½¿å®ƒè¾“å‡ºçš„ç‰¹å¾å°½å¯èƒ½ä¸åŸå§‹æ‰©æ•£æ¨¡å‹æ‰€æœ‰ timestep çš„ç‰¹å¾å¯¹é½ï¼ˆå¯èƒ½ç”¨ MSE æˆ– contrastive lossï¼‰

#### 4. ğŸ“ˆ å®éªŒç»“æœä¸æ€§èƒ½æå‡
- ä¸‹æ¸¸ä»»åŠ¡åŒ…æ‹¬ï¼š
  - è¯­ä¹‰åŒ¹é…ï¼ˆsemantic correspondence matchingï¼‰
  - å•ç›®æ·±åº¦ä¼°è®¡
  - è¯­ä¹‰åˆ†å‰²ã€å›¾åƒåˆ†ç±»ç­‰
- åœ¨æ‰€æœ‰ä»»åŠ¡ä¸Šï¼Œå‡ä¼˜äºåŸå§‹æ‰©æ•£ç‰¹å¾
- å¯†é›†ä»»åŠ¡ï¼ˆå¦‚è¯­ä¹‰åŒ¹é…ï¼‰æ•ˆæœæå‡æœ€æ˜¾è‘—ï¼Œè¾¾åˆ°æ–°çš„ SOTA æ°´å¹³

#### 5. âš¡ æ— éœ€ensembleï¼Œé€Ÿåº¦å¿« 8å€+
- ä¸å†éœ€è¦åœ¨ä¸åŒ timestep æˆ–ä¸åŒå™ªå£°ä¸Š ensembleï¼ˆä¾‹å¦‚ DIFT åšçš„ï¼‰
- æ¨ç†æ•ˆç‡å¤§å¤§æå‡ï¼ˆä¾‹å¦‚ï¼šæ¯” DIFT å¿«çº¦ 8 å€ï¼‰
- åŒæ—¶æ€§èƒ½æ›´å¥½

### âœ… ä¸‰ã€ä»·å€¼ä¸é€‚ç”¨èŒƒå›´
| é¡¹ç›®       | è¯´æ˜                                      |
| -------- | --------------------------------------- |
| ğŸ§© é€‚åˆä»»åŠ¡  | æ‰€æœ‰è§†è§‰ä¸‹æ¸¸ä»»åŠ¡ï¼Œç‰¹åˆ«æ˜¯**å¯†é›†é¢„æµ‹ç±»ä»»åŠ¡**                 |
| ğŸ› ï¸ é€‚é…èƒ½åŠ› | å…¼å®¹é¢„è®­ç»ƒ diffusion backboneï¼Œå¯ä¸ DINOv2 ç‰¹å¾èåˆ |
| ğŸš€ æ¨ç†é€Ÿåº¦  | å•ä¸€å‰å‘ï¼Œæ— éœ€å™ªå£°/å¤štimestepï¼Œå¤§å¹…æé€Ÿ                |
| ğŸ¯ è®­ç»ƒæˆæœ¬  | éå¸¸è½»é‡ï¼ˆå•å¼ A100 GPUï¼Œ30åˆ†é’Ÿï¼‰                   |

-----------------
Our main contributions are as follows:
1. We propose CleanDIFT, a finetuning approach for diffusion models that enables them to **operate on clean images** and makes the inherent world knowledge of these models more accessible.
2. We show how to **consolidate information from all diffusion timesteps into a single feature prediction, removing the need for task-specific timestep tuning**.
3. We demonstrate significant performance gains of our diffusion feature extraction technique across a wide range of down-stream tasks, notably surpassing the current state of the art in zero-shot unsupervised semantic correspondence detection. We further demonstrate the generality of our enhanced features by showing that these performance gains transfer to advanced methods that fuse diffusion features or operate in a supervised setting.
4. Our proposed approach is significantly more efficient than previous methods that tried to address this problem by noise ensembling or supervised training.
-----------------------
| ç¼–å·    | è´¡çŒ®å†…å®¹                                              | è¯´æ˜                                  |
| ----- | ------------------------------------------------- | ----------------------------------- |
| **1** | æå‡º CleanDIFT æ–¹æ³•ï¼Œä½¿æ‰©æ•£æ¨¡å‹èƒ½ç›´æ¥ä½œç”¨äº**å¹²å‡€å›¾åƒ**               | æ‰“ç ´â€œå¿…é¡»åŠ å™ªæ‰èƒ½æå–æœ‰ç”¨ç‰¹å¾â€çš„é™åˆ¶ï¼Œä½¿æ¨¡å‹å†…åœ¨ä¸–ç•ŒçŸ¥è¯†æ›´æ˜“è®¿é—®   |
| **2** | å°†æ‰€æœ‰ timestep çš„ä¿¡æ¯**æ•´åˆä¸ºå•ä¸€ç‰¹å¾è¾“å‡º**ï¼Œæ— éœ€ä»»åŠ¡ç‰¹å®šçš„ timestep è°ƒå‚ | æ¶ˆé™¤å¯¹ timestep çš„æ•æ„Ÿæ€§ï¼Œæé«˜ç‰¹å¾çš„**é€šç”¨æ€§ä¸ç¨³å®šæ€§**  |
| **3** | åœ¨å¤šä¸ªä¸‹æ¸¸ä»»åŠ¡ä¸­å–å¾—**æ˜¾è‘—æ€§èƒ½æå‡**ï¼Œç‰¹åˆ«æ˜¯åœ¨**é›¶æ ·æœ¬æ— ç›‘ç£è¯­ä¹‰åŒ¹é…ä»»åŠ¡ä¸­è¶…è¶ŠSOTA**  | åŒæ—¶éªŒè¯æ‰€æç‰¹å¾åœ¨**èåˆæ–¹æ³•å’Œæœ‰ç›‘ç£è®¾ç½®ä¸­å…·æœ‰è¿ç§»æ€§å’Œå¹¿æ³›é€‚ç”¨æ€§** |
| **4** | ç›¸æ¯”äºä¾èµ– **noise ensemble æˆ–æœ‰ç›‘ç£è®­ç»ƒ** çš„æ–¹æ³•ï¼Œ**æ›´é«˜æ•ˆã€æ›´è½»é‡**   | å¾®è°ƒè¿‡ç¨‹çŸ­ï¼ˆå¦‚30åˆ†é’Ÿ/A100ï¼‰ï¼Œæ¨ç†é€Ÿåº¦æ›´å¿«ï¼Œéƒ¨ç½²æˆæœ¬æ›´ä½     |

----------------------
## Related Work
### Self-Supervised Representation Learning
- Features from large, pre-trained foundation models have been shown to yield competitive performance to supervised models for a variety of downstream tasks, both in zero-shot and
fine-tuning settings [20, 43, 48]. These foundation models are trained on different pre-text tasks like inpainting [20], predicting transformations [19], patch reordering [38, 42],
and discriminative tasks [6, 43]. 
- DINOv2 [43] uses a discriminative objective combined with self-distillation to learn general-purpose visual features that have proven useful for a variety of downstream tasks [10]. CLIP [48] learns such features by employing a contrastive objective on text-image pairs. Masked Autoencoders [20] (MAEs) are trained to reconstruct masked out patches of the input, also resulting in general-purpose visual features.

### Diffusion Models as Self-Supervised Learners 
- Diffusion models [24, 60, 61] are generative models that have defined the state-of-the-art in image generation [12, 14, 41,46, 51, 53], video generation [13, 47], and audio generation [15, 30] in recent years. Their primary purpose is to generate high-quality samples (images, videos, etc.). However, generation can also be interpreted as a pretext task for learning expressive features, since the model has to build up comprehensive world knowledge in order to generate
plausible samples [9, 17, 27, 32, 62].
- Features from diffusion models (typically referred to as diffusion features) are obtained by passing a noised image through the diffusion model and extracting intermediate feature representations.
- They have been shown to be useful for a variety of tasks such as finding semantic correspondences [21, 35, 62], semantic and panoptic segmentation [2, 39, 63, 64], classification [31], and object detection [8]. For semantic correspondence matching, features are leveraged to identify semantically matching regions across images. Existing approaches utilize diffusion features either in a zero-shot setting [21, 62] or fine-tune them for the semantic correspondence task [33, 35, 67]. Some zero-shot approaches do not fine-tune on semantic correspondence but still require tuning a prompt to activate attention maps at
the query location of the correspondence [21].
- In contrast, our approach aims to provide universal features usable for various down-stream tasks in a true zero-shot manner. Further, diffusion features have been shown to complement features from other self-supervised learning methods such as DINOv2 [18, 43, 66]. DINOv2 features provide sparse but accurate semantic information, while Diffusion features provide dense spatial information albeit with sometimes inaccurate semantics. State-of-the-art approaches for semantic correspondence detection exploit this and fuse features [66, 67]. Compared to DINOv2 features, diffusion features yield smoother, spatially more coherent correspondences [66].

### Distillation for Diffusion Models
- Knowledge Distillation [22] is a technique used to distill knowledge from a teacher model into a student model.
- In the context of diffusion models, this is typically applied either to reduce the required denoising steps [54] or to distill a classifier-free guided [23] model into one without CFG [36]. While our approach is inspired by distillation, we consolidate features from T different models, with T being the number of discrete diffusion timesteps, because the features are different at every timestep.
----------------------
## ç›¸å…³å·¥ä½œ
### ğŸ“‹ è‡ªç›‘ç£è¡¨ç¤ºå­¦ä¹ ã€æ‰©æ•£æ¨¡å‹ç‰¹å¾ä¸è’¸é¦æŠ€æœ¯æ€»ç»“è¡¨
| ç±»åˆ«                                                        | å†…å®¹æ€»ç»“                                                                                                 | ä»£è¡¨æ–¹æ³•ä¸ç‰¹ç‚¹                                                                                             |
| --------------------------------------------------------- | ---------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------- |
| ğŸ¯ **è‡ªç›‘ç£è§†è§‰ç‰¹å¾å­¦ä¹ ï¼ˆSelf-Supervised Representation Learningï¼‰** | åˆ©ç”¨æ— ç›‘ç£çš„**é¢„è®­ç»ƒä»»åŠ¡**ï¼ˆpre-text taskï¼‰å­¦ä¹ é€šç”¨è§†è§‰ç‰¹å¾ï¼Œå¯ç”¨äº zero-shot æˆ– fine-tuning çš„ä¸‹æ¸¸ä»»åŠ¡                             | - **DINOv2**ï¼šåˆ¤åˆ«å¼ç›®æ ‡ + è‡ªè’¸é¦ï¼Œç‰¹å¾ç¨€ç–ä½†è¯­ä¹‰ç²¾ç¡®<br>- **CLIP**ï¼šæ–‡æœ¬-å›¾åƒå¯¹ä¸Šçš„å¯¹æ¯”å­¦ä¹ ï¼Œè·¨æ¨¡æ€<br>- **MAE**ï¼šé®æŒ¡é¢„æµ‹ï¼ˆpatch é‡å»ºï¼‰ï¼Œç»“æ„æ„ŸçŸ¥å¼º |
| ğŸŒ€ **æ‰©æ•£æ¨¡å‹ä½œä¸ºè‡ªç›‘ç£å­¦ä¹ å™¨ï¼ˆDiffusion Models as SSL Learnersï¼‰**     | æ‰©æ•£æ¨¡å‹åŸä¸ºç”Ÿæˆä»»åŠ¡è®¾è®¡ï¼Œä½†å…¶ç”Ÿæˆä»»åŠ¡æœ¬è´¨ä¸Šéœ€è¦ä¸°å¯Œçš„ä¸–ç•ŒçŸ¥è¯† â†’ å¯ä»¥çœ‹ä½œä¸€ç§éšå¼çš„â€œé¢„è®­ç»ƒä»»åŠ¡â€ï¼Œå› æ­¤å…¶\*\*ä¸­é—´å±‚ç‰¹å¾ï¼ˆdiffusion featuresï¼‰\*\*å…·æœ‰é€šç”¨æ€§         | - ç‰¹å¾é€šè¿‡ **è¾“å…¥åŠ å™ªå›¾åƒï¼Œæå–ä¸­é—´å±‚è¡¨ç¤º**<br>- å¹¿æ³›ç”¨äºï¼šè¯­ä¹‰åŒ¹é…ã€å›¾åƒåˆ†å‰²ã€ç›®æ ‡æ£€æµ‹ã€åˆ†ç±»ç­‰<br>- åœ¨è¯­ä¹‰åŒ¹é…ä¸­å¯æä¾›**ç©ºé—´è¿è´¯æ€§å¼ºä½†è¯­ä¹‰åå·®**çš„ç¨ å¯†ç‰¹å¾           |
| ğŸ”€ **ä¸å…¶ä»–è‡ªç›‘ç£ç‰¹å¾çš„äº’è¡¥æ€§**                                       | Diffusion ç‰¹å¾ + DINOv2 ç‰¹å¾å¸¸èåˆä½¿ç”¨ï¼š<br>Diffusion â†’ ç¨ å¯†ç©ºé—´ç»“æ„å¥½<br>DINOv2 â†’ ç¨€ç–è¯­ä¹‰ä¿¡æ¯å‡†                            | - SOTA è¯­ä¹‰åŒ¹é…æ–¹æ³• \[66, 67] å°±èåˆäº†ä¸¤è€…<br>- Diffusion ç‰¹å¾å¸¦æ¥**å¹³æ»‘ä¸”è¿è´¯çš„åŒºåŸŸå¯¹é½èƒ½åŠ›**                                  |
| ğŸ”§ **æ‰©æ•£æ¨¡å‹ä¸­çš„è’¸é¦ï¼ˆDistillation for Diffusion Modelsï¼‰**        | è’¸é¦å¸¸ç”¨äºï¼š<br>â‘  å‡å°‘å»å™ªæ­¥éª¤æ•°é‡<br>â‘¡ ç§»é™¤ classifier-free guidance (CFG) æœºåˆ¶<br>â‘¢ï¼ˆæœ¬æ–‡ï¼‰å°† T ä¸ª timestep ç‰¹å¾æ•´åˆæˆä¸€ä¸ªç»Ÿä¸€ç‰¹å¾æå–å™¨ | - ä¼ ç»Ÿè’¸é¦ï¼šç”¨äºåŠ é€Ÿæ¨ç†æˆ–å»é™¤æ§åˆ¶å™¨<br>- æœ¬æ–‡è’¸é¦ï¼š**å°†ä¸åŒ timestep çš„æ¨¡å‹ç‰¹å¾æ•´åˆæˆç»Ÿä¸€æ¨¡å‹ï¼ˆè·¨æ—¶åˆ»ç‰¹å¾å¯¹é½ï¼‰**                                |

### ğŸ“Œ å…³é”®è¯å¯¹æ¯”ç®€æ
| ç‰¹å¾        | Diffusion Features    | DINOv2 Features          |
| --------- | --------------------- | ------------------------ |
| **ç¨ å¯†æ€§**   | âœ… ç©ºé—´ç»“æ„ç¨ å¯†              | âŒ ç¨€ç–å…³é”®ç‚¹è¯­ä¹‰                |
| **è¯­ä¹‰å‡†ç¡®æ€§** | â›”ï¸ è¯­ä¹‰å¯èƒ½ä¸ç²¾ç¡®            | âœ… ç²¾å‡†è¯­ä¹‰å¯¹é½                 |
| **å¯è¿ç§»æ€§**  | å¯ç”¨äº dense taskï¼Œå¤šä»»åŠ¡æ³›åŒ–å¥½ | è¾ƒå¼ºä½†åœ¨ dense task ä¸­æœ‰é™      |
| **ç»„åˆæ–¹å¼**  | å¸¸ç”¨äº **ç‰¹å¾èåˆ**          | ä¸ diffusion feature äº’è¡¥ä½¿ç”¨ |



